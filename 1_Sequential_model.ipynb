{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from random import randint\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels = []\n",
    "train_samples = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create a fake dataset to play with. Imagine a drug trial\n",
    "for i in range(50):\n",
    "    random_younger = randint(13,64) #participants with age less than 65\n",
    "    train_samples.append(random_younger)\n",
    "    train_labels.append(1) #young with side effects\n",
    "    \n",
    "    random_older = randint(65,100)\n",
    "    train_samples.append(random_older)\n",
    "    train_labels.append(0) #old without side effects\n",
    "    \n",
    "for i in range(1000):\n",
    "    random_younger = randint(13,64) #participants with age less than 65\n",
    "    train_samples.append(random_younger)\n",
    "    train_labels.append(0) #young without side effects\n",
    "    \n",
    "    random_older = randint(65,100)\n",
    "    train_samples.append(random_older)\n",
    "    train_labels.append(1) #old with side effects\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Get ready to call Keras models\n",
    "## a list is not a valid input --> change to np.array\n",
    "\n",
    "train_labels = np.array(train_labels)\n",
    "train_samples = np.array(train_samples)\n",
    "train_labels, train_samples = shuffle(train_labels, train_samples)\n",
    "\n",
    "## Scale the data (to better Gradient Descent performaces)\n",
    "scaler = MinMaxScaler(feature_range=(0,1))\n",
    "## Semp r solt cacate : fit_transform si aspetta una certa shape, e quello è il senso di reshape\n",
    "scaled_train_samples = scaler.fit_transform(train_samples.reshape(-1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Ok, now we have created our fake data. Let's write a simple model that works on them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Activation, Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.metrics import categorical_crossentropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solo se giri con una GPU\n",
    "# physical_devices = tf.config.experimental.list_physical_devices(\"GPU\")\n",
    "# print(\"Num GPUs available: \", len(physical_devices))\n",
    "# print(physical_devices)\n",
    "# tf.config.experimental.set_memory_growth(physical_devices[0], True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 16)                32        \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 32)                544       \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 2)                 66        \n",
      "=================================================================\n",
      "Total params: 642\n",
      "Trainable params: 642\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "(2100, 1)\n"
     ]
    }
   ],
   "source": [
    "## Ok, let's architect this model\n",
    "model = Sequential([\n",
    "    Dense(units=16, input_shape=(1,),activation=\"relu\"),\n",
    "    Dense(units=32, activation=\"relu\"),\n",
    "    Dense(units=2, activation=\"softmax\") #output layer (the two possible output class)\n",
    "    ])\n",
    "\n",
    "model.summary()\n",
    "\n",
    "# Notes:\n",
    "# Dense means fully connected layer\n",
    "# Nel primo layer devi specificare lo shape dei dati in input. In questo caso noi passiamo (1,).\n",
    "# Mo che significa (1,) ? Io so che passo un training set, di cui ho fatto il reshape(-1,1) \n",
    "print(scaled_train_samples.shape) # --> (2100,1)\n",
    "# Sostanzialmente, dal vettore colonna che avevo, ora ho una \"matrice\" di 2100 righe e 1 colonna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=Adam(learning_rate=0.0001), loss=\"sparse_categorical_crossentropy\", metrics=['accuracy'])\n",
    "\n",
    "## Loss = cosa stai minimizzando\n",
    "## metrics = come stai misurando la bontà del modello"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "210/210 - 0s - loss: 0.6830 - accuracy: 0.5371\n",
      "Epoch 2/30\n",
      "210/210 - 0s - loss: 0.6607 - accuracy: 0.6143\n",
      "Epoch 3/30\n",
      "210/210 - 0s - loss: 0.6331 - accuracy: 0.7195\n",
      "Epoch 4/30\n",
      "210/210 - 0s - loss: 0.6052 - accuracy: 0.7438\n",
      "Epoch 5/30\n",
      "210/210 - 0s - loss: 0.5756 - accuracy: 0.7738\n",
      "Epoch 6/30\n",
      "210/210 - 0s - loss: 0.5451 - accuracy: 0.8033\n",
      "Epoch 7/30\n",
      "210/210 - 0s - loss: 0.5140 - accuracy: 0.8338\n",
      "Epoch 8/30\n",
      "210/210 - 0s - loss: 0.4836 - accuracy: 0.8500\n",
      "Epoch 9/30\n",
      "210/210 - 0s - loss: 0.4548 - accuracy: 0.8633\n",
      "Epoch 10/30\n",
      "210/210 - 0s - loss: 0.4280 - accuracy: 0.8719\n",
      "Epoch 11/30\n",
      "210/210 - 0s - loss: 0.4038 - accuracy: 0.8867\n",
      "Epoch 12/30\n",
      "210/210 - 0s - loss: 0.3824 - accuracy: 0.8957\n",
      "Epoch 13/30\n",
      "210/210 - 0s - loss: 0.3638 - accuracy: 0.9014\n",
      "Epoch 14/30\n",
      "210/210 - 0s - loss: 0.3477 - accuracy: 0.9100\n",
      "Epoch 15/30\n",
      "210/210 - 0s - loss: 0.3338 - accuracy: 0.9133\n",
      "Epoch 16/30\n",
      "210/210 - 0s - loss: 0.3221 - accuracy: 0.9186\n",
      "Epoch 17/30\n",
      "210/210 - 0s - loss: 0.3118 - accuracy: 0.9186\n",
      "Epoch 18/30\n",
      "210/210 - 0s - loss: 0.3032 - accuracy: 0.9205\n",
      "Epoch 19/30\n",
      "210/210 - 0s - loss: 0.2957 - accuracy: 0.9257\n",
      "Epoch 20/30\n",
      "210/210 - 0s - loss: 0.2896 - accuracy: 0.9233\n",
      "Epoch 21/30\n",
      "210/210 - 0s - loss: 0.2841 - accuracy: 0.9262\n",
      "Epoch 22/30\n",
      "210/210 - 0s - loss: 0.2794 - accuracy: 0.9300\n",
      "Epoch 23/30\n",
      "210/210 - 0s - loss: 0.2756 - accuracy: 0.9281\n",
      "Epoch 24/30\n",
      "210/210 - 0s - loss: 0.2720 - accuracy: 0.9257\n",
      "Epoch 25/30\n",
      "210/210 - 0s - loss: 0.2691 - accuracy: 0.9362\n",
      "Epoch 26/30\n",
      "210/210 - 0s - loss: 0.2665 - accuracy: 0.9367\n",
      "Epoch 27/30\n",
      "210/210 - 0s - loss: 0.2640 - accuracy: 0.9338\n",
      "Epoch 28/30\n",
      "210/210 - 0s - loss: 0.2620 - accuracy: 0.9381\n",
      "Epoch 29/30\n",
      "210/210 - 0s - loss: 0.2601 - accuracy: 0.9362\n",
      "Epoch 30/30\n",
      "210/210 - 0s - loss: 0.2582 - accuracy: 0.9381\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7eff54607450>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x=scaled_train_samples, y=train_labels, batch_size=10, epochs=30, shuffle=True, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "189/189 - 0s - loss: 0.2533 - accuracy: 0.9360 - val_loss: 0.2893 - val_accuracy: 0.9286\n",
      "Epoch 2/30\n",
      "189/189 - 0s - loss: 0.2519 - accuracy: 0.9386 - val_loss: 0.2885 - val_accuracy: 0.9286\n",
      "Epoch 3/30\n",
      "189/189 - 0s - loss: 0.2507 - accuracy: 0.9376 - val_loss: 0.2871 - val_accuracy: 0.9286\n",
      "Epoch 4/30\n",
      "189/189 - 0s - loss: 0.2497 - accuracy: 0.9402 - val_loss: 0.2855 - val_accuracy: 0.9286\n",
      "Epoch 5/30\n",
      "189/189 - 0s - loss: 0.2483 - accuracy: 0.9376 - val_loss: 0.2856 - val_accuracy: 0.9286\n",
      "Epoch 6/30\n",
      "189/189 - 0s - loss: 0.2474 - accuracy: 0.9402 - val_loss: 0.2847 - val_accuracy: 0.9333\n",
      "Epoch 7/30\n",
      "189/189 - 0s - loss: 0.2465 - accuracy: 0.9402 - val_loss: 0.2836 - val_accuracy: 0.9286\n",
      "Epoch 8/30\n",
      "189/189 - 0s - loss: 0.2457 - accuracy: 0.9381 - val_loss: 0.2824 - val_accuracy: 0.9286\n",
      "Epoch 9/30\n",
      "189/189 - 0s - loss: 0.2449 - accuracy: 0.9402 - val_loss: 0.2821 - val_accuracy: 0.9286\n",
      "Epoch 10/30\n",
      "189/189 - 0s - loss: 0.2442 - accuracy: 0.9376 - val_loss: 0.2817 - val_accuracy: 0.9286\n",
      "Epoch 11/30\n",
      "189/189 - 0s - loss: 0.2435 - accuracy: 0.9423 - val_loss: 0.2811 - val_accuracy: 0.9333\n",
      "Epoch 12/30\n",
      "189/189 - 0s - loss: 0.2428 - accuracy: 0.9450 - val_loss: 0.2801 - val_accuracy: 0.9286\n",
      "Epoch 13/30\n",
      "189/189 - 0s - loss: 0.2424 - accuracy: 0.9402 - val_loss: 0.2798 - val_accuracy: 0.9286\n",
      "Epoch 14/30\n",
      "189/189 - 0s - loss: 0.2418 - accuracy: 0.9397 - val_loss: 0.2796 - val_accuracy: 0.9333\n",
      "Epoch 15/30\n",
      "189/189 - 0s - loss: 0.2413 - accuracy: 0.9429 - val_loss: 0.2793 - val_accuracy: 0.9333\n",
      "Epoch 16/30\n",
      "189/189 - 0s - loss: 0.2408 - accuracy: 0.9423 - val_loss: 0.2785 - val_accuracy: 0.9286\n",
      "Epoch 17/30\n",
      "189/189 - 0s - loss: 0.2404 - accuracy: 0.9423 - val_loss: 0.2781 - val_accuracy: 0.9286\n",
      "Epoch 18/30\n",
      "189/189 - 0s - loss: 0.2400 - accuracy: 0.9429 - val_loss: 0.2776 - val_accuracy: 0.9286\n",
      "Epoch 19/30\n",
      "189/189 - 0s - loss: 0.2395 - accuracy: 0.9418 - val_loss: 0.2775 - val_accuracy: 0.9333\n",
      "Epoch 20/30\n",
      "189/189 - 0s - loss: 0.2391 - accuracy: 0.9434 - val_loss: 0.2772 - val_accuracy: 0.9333\n",
      "Epoch 21/30\n",
      "189/189 - 0s - loss: 0.2388 - accuracy: 0.9418 - val_loss: 0.2768 - val_accuracy: 0.9333\n",
      "Epoch 22/30\n",
      "189/189 - 0s - loss: 0.2385 - accuracy: 0.9455 - val_loss: 0.2764 - val_accuracy: 0.9333\n",
      "Epoch 23/30\n",
      "189/189 - 0s - loss: 0.2381 - accuracy: 0.9450 - val_loss: 0.2758 - val_accuracy: 0.9286\n",
      "Epoch 24/30\n",
      "189/189 - 0s - loss: 0.2379 - accuracy: 0.9386 - val_loss: 0.2760 - val_accuracy: 0.9333\n",
      "Epoch 25/30\n",
      "189/189 - 0s - loss: 0.2376 - accuracy: 0.9413 - val_loss: 0.2759 - val_accuracy: 0.9333\n",
      "Epoch 26/30\n",
      "189/189 - 0s - loss: 0.2373 - accuracy: 0.9455 - val_loss: 0.2749 - val_accuracy: 0.9333\n",
      "Epoch 27/30\n",
      "189/189 - 0s - loss: 0.2371 - accuracy: 0.9455 - val_loss: 0.2747 - val_accuracy: 0.9333\n",
      "Epoch 28/30\n",
      "189/189 - 0s - loss: 0.2368 - accuracy: 0.9423 - val_loss: 0.2747 - val_accuracy: 0.9333\n",
      "Epoch 29/30\n",
      "189/189 - 0s - loss: 0.2365 - accuracy: 0.9434 - val_loss: 0.2745 - val_accuracy: 0.9333\n",
      "Epoch 30/30\n",
      "189/189 - 0s - loss: 0.2363 - accuracy: 0.9455 - val_loss: 0.2738 - val_accuracy: 0.9333\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7eff447d3690>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Fittare con un validation set\n",
    "#validation_split=0.1 significa che l'ultimo 10 % del training set è usato come validation_set\n",
    "model.fit(x=scaled_train_samples, y=train_labels, validation_split=0.1, batch_size=10, epochs=30, shuffle=True, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Ci inventiamo un test set a questo punto\n",
    "test_labels = []\n",
    "test_samples = []\n",
    "\n",
    "for i in range(10):\n",
    "    random_younger = randint(13,64) #participants with age less than 65\n",
    "    test_samples.append(random_younger)\n",
    "    test_labels.append(1) #young with side effects\n",
    "    \n",
    "    random_older = randint(65,100)\n",
    "    test_samples.append(random_older)\n",
    "    test_labels.append(0) #old without side effects\n",
    "    \n",
    "for i in range(200):\n",
    "    random_younger = randint(13,64) #participants with age less than 65\n",
    "    test_samples.append(random_younger)\n",
    "    test_labels.append(0) #young without side effects\n",
    "    \n",
    "    random_older = randint(65,100)\n",
    "    test_samples.append(random_older)\n",
    "    test_labels.append(1) #old with side effects\n",
    "    \n",
    "## Convert to np array data format\n",
    "test_labels = np.array(test_labels)\n",
    "test_samples = np.array(test_samples)\n",
    "test_labels, test_samples = shuffle(test_labels, test_samples)\n",
    "\n",
    "## MinMax scaling on the reshaped test sample\n",
    "scaled_test_samples = scaler.fit_transform(test_samples.reshape(-1,1))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict\n",
    "predictions = model.predict(x=scaled_test_samples, batch_size=10, verbose=0)\n",
    "\n",
    "# for i in predictions:\n",
    "#     print(i)\n",
    "#     # print(i[0] + i[1]) # a somma praticamente 1.\n",
    "    \n",
    "rounded_preductions = np.argmax(predictions, axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[194  16]\n",
      " [ 10 200]]\n"
     ]
    }
   ],
   "source": [
    "## Confusion matrix\n",
    "%matplotlib inline\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import itertools\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "cm = confusion_matrix(y_true=test_labels, y_pred=rounded_preductions)\n",
    "\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Save and load the model: ci sono 3 diverse possibilità, ognuna con vantaggi e svantaggi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 16)                32        \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 32)                544       \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 2)                 66        \n",
      "=================================================================\n",
      "Total params: 642\n",
      "Trainable params: 642\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.optimizer_v2.adam.Adam at 0x7eff1819ac90>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1. model.save()\n",
    "import os.path\n",
    "if os.path.isfile(\"./medical_trail_model.h5\") is False:\n",
    "    model.save(\"./medical_trail.h5\")\n",
    "    \n",
    "## Questo metodo è quello più \"comprehensive\". Salva tutto:\n",
    "# l'architettura\n",
    "# pesi\n",
    "# training configuration: loss, optimizer\n",
    "# the state of the optimizer: you can resume training exactly where you left off\n",
    "\n",
    "from tensorflow.keras.models import load_model\n",
    "new_model = load_model(\"./medical_trail.h5\")\n",
    "new_model.summary()\n",
    "new_model.get_weights() #in case you want to inspect the weights\n",
    "new_model.optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 16)                32        \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 32)                544       \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 2)                 66        \n",
      "=================================================================\n",
      "Total params: 642\n",
      "Trainable params: 642\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# 2. model.to_json()\n",
    "json_string = model.to_json()\n",
    "\n",
    "## Salva soltanto l'architettura del modello, non i pesi, né le sue training configuration\n",
    "\n",
    "from tensorflow.keras.models import model_from_json\n",
    "model_architecture = model_from_json(json_string)\n",
    "model_architecture.summary()\n",
    "## Ma va ri-trainato, niente è salvato. Serve solo per non perdere l'architettura del modello."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. model.save_weights()\n",
    "import os.path\n",
    "if os.path.isfile(\"./my_model_weigths.h5\") is False:\n",
    "    model.save_weights(\"./my_model_weights.h5\")\n",
    "    \n",
    "## Siccome questo salva SOLO i pesi, se voglio fare \"una prediction\", beh, devo rifarmelo\n",
    "model2 = Sequential([\n",
    "    Dense(units=16, input_shape=(1,), activation=\"relu\"),\n",
    "    Dense(units=32, activation=\"relu\"),\n",
    "    Dense(units=2, activation=\"softmax\")\n",
    "    ])\n",
    "model2.load_weights(\"./my_model_weights.h5\") # e gli assegni i pesi che hai salvato prima"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
